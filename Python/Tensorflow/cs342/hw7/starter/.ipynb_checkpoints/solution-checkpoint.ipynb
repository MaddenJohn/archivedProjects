{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "This homework focuses on fully convolutional networks.\n",
    "\n",
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import util\n",
    "\n",
    "# Colors to visualize the labeling\n",
    "COLORS = np.array([(0,0,0), (255,0,0), (0,255,0), (255,255,0), (0,0,255), (255,255,255)], dtype=np.uint8)\n",
    "CROP_SIZE = 64\n",
    "\n",
    "def parser(record):\n",
    "    # Parse the TF record\n",
    "    parsed = tf.parse_single_example(record, features={\n",
    "        'height': tf.FixedLenFeature([], tf.int64),\n",
    "        'width': tf.FixedLenFeature([], tf.int64),\n",
    "        'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "        'label_raw': tf.FixedLenFeature([], tf.string)\n",
    "    })\n",
    "    # Load the data and format it\n",
    "    H = tf.cast(parsed['height'], tf.int32)\n",
    "    W = tf.cast(parsed['width'], tf.int32)\n",
    "    image = tf.reshape(tf.decode_raw(parsed[\"image_raw\"], tf.uint8), [H,W,3])\n",
    "    label = tf.reshape(tf.decode_raw(parsed[\"label_raw\"], tf.uint8), [H,W])\n",
    "    \n",
    "    ## Data augmentation\n",
    "    # Stack the image and labels to make sure the same operations are applied\n",
    "    data = tf.concat([image, label[:,:,None]], axis=-1)\n",
    "    \n",
    "    # TODO: Apply the data augmentation (you should both crop the images randomly and flip them)\n",
    "    data = tf.random_crop(data, [CROP_SIZE, CROP_SIZE, 4])\n",
    "    data = tf.image.random_flip_left_right(data)\n",
    "    \n",
    "    return data[:,:,:-1], data[:,:,-1]\n",
    "\n",
    "def load_dataset(tfrecord):\n",
    "    # Load the dataset\n",
    "    dataset = tf.contrib.data.TFRecordDataset(tfrecord)\n",
    "\n",
    "    # Parse the tf record entries\n",
    "    dataset = dataset.map(parser, num_threads=8, output_buffer_size=1024)\n",
    "\n",
    "    # Shuffle the data, batch it and run this for multiple epochs\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.repeat()\n",
    "    return dataset\n",
    "\n",
    "# We still have 6 classes\n",
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Define your convnet\n",
    "Important note. The label frequency is horribly inbalanced for this task. On the training set\n",
    "```[ 0.66839117, 0.00382957, 0.00092516, 0.00345217, 0.00339063, 0.3200113 ]```\n",
    "On the validation set\n",
    "```[ 0.68367316, 0.00392016, 0.00165766, 0.00194697, 0.0034067, 0.30539535]```\n",
    "Tux, bonus, objects and enemies make up less than 1.5% of all labels overall.\n",
    "You should reweight the loss to address this, if you don't your model will likely ignore all but background and tile labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of variables used  496449\n"
     ]
    }
   ],
   "source": [
    "# Create a new log directory (if you run low on disk space you can either disable this or delete old logs)\n",
    "# run: `tensorboard --logdir log` to see all the nice summaries\n",
    "for n_model in range(1000):\n",
    "    LOG_DIR = 'log/model_%d'%n_model\n",
    "    from os import path\n",
    "    if not path.exists(LOG_DIR):\n",
    "        break\n",
    "\n",
    "# Lets clear the tensorflow graph, so that you don't have to restart the notebook every time you change the network\n",
    "tf.reset_default_graph()\n",
    "\n",
    "TF_COLORS = tf.constant(COLORS)\n",
    "\n",
    "train_data = load_dataset('train.tfrecord')\n",
    "valid_data = load_dataset('valid.tfrecord')\n",
    "\n",
    "# Create an iterator for the datasets\n",
    "# The iterator allows us to quickly switch between training and validataion\n",
    "iterator = tf.contrib.data.Iterator.from_structure(train_data.output_types, ((None,None,None,3), (None,None,None)))\n",
    "\n",
    "# and fetch the next images from the dataset (every time next_image is evaluated a new image set of 32 images is returned)\n",
    "next_image, next_label = iterator.get_next()\n",
    "\n",
    "# Define operations that switch between train and valid\n",
    "switch_train_op = iterator.make_initializer(train_data)\n",
    "switch_valid_op = iterator.make_initializer(valid_data)\n",
    "\n",
    "# Convert the input\n",
    "image = tf.cast(next_image, tf.float32)\n",
    "label = tf.cast(next_label, tf.int32)\n",
    "\n",
    "# Whiten the input\n",
    "inputs = tf.identity(image, name='inputs')\n",
    "white_inputs = (inputs - 100.) / 72.\n",
    "\n",
    "# TODO: Define your convnet here\n",
    "C0 = 25\n",
    "D = 5\n",
    "h = white_inputs\n",
    "hs = []\n",
    "for i in range(D):\n",
    "    hs.append(h)\n",
    "    h = tf.contrib.layers.conv2d(h, C0*int(1.5**i), (3,3), stride=2, scope='conv%d'%(i+1))\n",
    "\n",
    "for i in range(D)[::-1]:\n",
    "    h = tf.contrib.layers.conv2d_transpose(h, C0*int(1.5**i), (3,3), stride=2, scope='upconv%d'%(i+1))\n",
    "    h = tf.concat([h, hs[i]], axis=-1)\n",
    "h = tf.contrib.layers.conv2d(h, num_classes, (1,1), scope='cls', activation_fn=None)\n",
    "\n",
    "# Let's compute the output labeling\n",
    "output = tf.identity(tf.argmax(h, axis=-1), name='output')\n",
    "\n",
    "# Define the loss function\n",
    "loss_weight = tf.constant([ 0.66839117, 0.00382957, 0.00092516, 0.00345217, 0.00339063, 0.3200113 ]) ** -0.9 + 1\n",
    "weight = tf.gather_nd(loss_weight,label[:,:,:,None])\n",
    "loss = tf.reduce_sum(weight * tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=label)) / tf.reduce_sum(weight)\n",
    "\n",
    "# Let's weight the regularization loss down, otherwise it will hurt the model performance\n",
    "# You can tune this weight if you wish\n",
    "regularization_loss = tf.losses.get_regularization_loss()\n",
    "total_loss = loss + 1e-6 * regularization_loss\n",
    "\n",
    "# Adam will likely converge much faster than SGD for this assignment.\n",
    "optimizer = tf.train.AdamOptimizer(0.001, 0.9, 0.999)\n",
    "\n",
    "# use that optimizer on your loss function (control_dependencies makes sure any \n",
    "# batch_norm parameters are properly updated)\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    opt = optimizer.minimize(total_loss)\n",
    "confusion = tf.confusion_matrix(labels=tf.reshape(label,[-1]), predictions=tf.reshape(output,[-1]), num_classes=num_classes)\n",
    "\n",
    "# Let's define some summaries for tensorboard\n",
    "colored_label = tf.gather_nd(TF_COLORS, label[:,:,:,None])\n",
    "colored_output = tf.gather_nd(TF_COLORS, output[:,:,:,None])\n",
    "tf.summary.image('confusion', tf.cast(confusion[None,:,:,None], tf.float32), max_outputs=1)\n",
    "tf.summary.image('image', next_image, max_outputs=3)\n",
    "tf.summary.image('label', colored_label, max_outputs=3)\n",
    "tf.summary.image('output', colored_output, max_outputs=3)\n",
    "tf.summary.scalar('loss', tf.placeholder(tf.float32, name='loss'))\n",
    "tf.summary.scalar('accuracy', tf.placeholder(tf.float32, name='accuracy'))\n",
    "tf.summary.scalar('class_accuracy', tf.placeholder(tf.float32, name='class_accuracy'))\n",
    "tf.summary.scalar('jaccard', tf.placeholder(tf.float32, name='jaccard'))\n",
    "tf.summary.scalar('val_accuracy', tf.placeholder(tf.float32, name='val_accuracy'))\n",
    "tf.summary.scalar('val_class_accuracy', tf.placeholder(tf.float32, name='val_class_accuracy'))\n",
    "tf.summary.scalar('val_jaccard', tf.placeholder(tf.float32, name='val_jaccard'))\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR, tf.get_default_graph())\n",
    "\n",
    "# Let's compute the model size\n",
    "print( \"Total number of variables used \", np.sum([v.get_shape().num_elements() for v in tf.trainable_variables()]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training\n",
    "\n",
    "Training might take up to 20 min depending on your architecture (and if you have a GPU or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0] Loss: 1.764  \t  A.: 0.126  CA.: 0.197  J.: 0.035  \t  Val A.: 0.299  CA.: 0.198  J.: 0.075\n",
      "[  1] Loss: 1.666  \t  A.: 0.483  CA.: 0.231  J.: 0.108  \t  Val A.: 0.651  CA.: 0.218  J.: 0.132\n",
      "[  2] Loss: 1.511  \t  A.: 0.622  CA.: 0.281  J.: 0.132  \t  Val A.: 0.538  CA.: 0.260  J.: 0.105\n",
      "[  3] Loss: 1.520  \t  A.: 0.607  CA.: 0.283  J.: 0.128  \t  Val A.: 0.784  CA.: 0.305  J.: 0.157\n",
      "[  4] Loss: 1.393  \t  A.: 0.610  CA.: 0.323  J.: 0.139  \t  Val A.: 0.607  CA.: 0.321  J.: 0.131\n",
      "[  5] Loss: 1.452  \t  A.: 0.533  CA.: 0.329  J.: 0.116  \t  Val A.: 0.702  CA.: 0.232  J.: 0.136\n",
      "[  6] Loss: 1.397  \t  A.: 0.570  CA.: 0.338  J.: 0.135  \t  Val A.: 0.518  CA.: 0.357  J.: 0.134\n",
      "[  7] Loss: 1.417  \t  A.: 0.587  CA.: 0.352  J.: 0.145  \t  Val A.: 0.716  CA.: 0.317  J.: 0.156\n",
      "[  8] Loss: 1.384  \t  A.: 0.638  CA.: 0.366  J.: 0.151  \t  Val A.: 0.593  CA.: 0.241  J.: 0.132\n",
      "[  9] Loss: 1.354  \t  A.: 0.603  CA.: 0.387  J.: 0.142  \t  Val A.: 0.589  CA.: 0.311  J.: 0.137\n",
      "[ 10] Loss: 1.329  \t  A.: 0.591  CA.: 0.438  J.: 0.169  \t  Val A.: 0.566  CA.: 0.415  J.: 0.154\n",
      "[ 11] Loss: 1.202  \t  A.: 0.632  CA.: 0.456  J.: 0.185  \t  Val A.: 0.644  CA.: 0.272  J.: 0.137\n",
      "[ 12] Loss: 1.170  \t  A.: 0.670  CA.: 0.462  J.: 0.195  \t  Val A.: 0.660  CA.: 0.452  J.: 0.176\n",
      "[ 13] Loss: 1.192  \t  A.: 0.659  CA.: 0.486  J.: 0.204  \t  Val A.: 0.625  CA.: 0.390  J.: 0.150\n",
      "[ 14] Loss: 1.094  \t  A.: 0.702  CA.: 0.485  J.: 0.210  \t  Val A.: 0.584  CA.: 0.331  J.: 0.158\n",
      "[ 15] Loss: 1.116  \t  A.: 0.718  CA.: 0.501  J.: 0.214  \t  Val A.: 0.677  CA.: 0.515  J.: 0.211\n",
      "[ 16] Loss: 1.040  \t  A.: 0.697  CA.: 0.523  J.: 0.220  \t  Val A.: 0.670  CA.: 0.336  J.: 0.164\n",
      "[ 17] Loss: 1.025  \t  A.: 0.706  CA.: 0.498  J.: 0.210  \t  Val A.: 0.693  CA.: 0.456  J.: 0.221\n",
      "[ 18] Loss: 1.013  \t  A.: 0.721  CA.: 0.520  J.: 0.226  \t  Val A.: 0.779  CA.: 0.536  J.: 0.254\n",
      "[ 19] Loss: 1.013  \t  A.: 0.703  CA.: 0.550  J.: 0.228  \t  Val A.: 0.652  CA.: 0.397  J.: 0.187\n",
      "[ 20] Loss: 0.943  \t  A.: 0.708  CA.: 0.558  J.: 0.217  \t  Val A.: 0.823  CA.: 0.586  J.: 0.297\n",
      "[ 21] Loss: 1.123  \t  A.: 0.743  CA.: 0.523  J.: 0.236  \t  Val A.: 0.760  CA.: 0.440  J.: 0.247\n",
      "[ 22] Loss: 1.009  \t  A.: 0.734  CA.: 0.566  J.: 0.239  \t  Val A.: 0.685  CA.: 0.557  J.: 0.248\n",
      "[ 23] Loss: 0.990  \t  A.: 0.759  CA.: 0.564  J.: 0.258  \t  Val A.: 0.715  CA.: 0.472  J.: 0.255\n",
      "[ 24] Loss: 1.019  \t  A.: 0.720  CA.: 0.559  J.: 0.247  \t  Val A.: 0.732  CA.: 0.505  J.: 0.272\n",
      "[ 25] Loss: 0.915  \t  A.: 0.772  CA.: 0.587  J.: 0.260  \t  Val A.: 0.716  CA.: 0.511  J.: 0.278\n",
      "[ 26] Loss: 0.947  \t  A.: 0.744  CA.: 0.583  J.: 0.258  \t  Val A.: 0.745  CA.: 0.534  J.: 0.260\n",
      "[ 27] Loss: 0.919  \t  A.: 0.761  CA.: 0.543  J.: 0.247  \t  Val A.: 0.797  CA.: 0.582  J.: 0.331\n",
      "[ 28] Loss: 0.822  \t  A.: 0.742  CA.: 0.601  J.: 0.285  \t  Val A.: 0.621  CA.: 0.535  J.: 0.241\n",
      "[ 29] Loss: 0.967  \t  A.: 0.770  CA.: 0.563  J.: 0.262  \t  Val A.: 0.738  CA.: 0.615  J.: 0.287\n",
      "[ 30] Loss: 0.896  \t  A.: 0.753  CA.: 0.588  J.: 0.269  \t  Val A.: 0.820  CA.: 0.582  J.: 0.315\n",
      "[ 31] Loss: 0.808  \t  A.: 0.764  CA.: 0.613  J.: 0.273  \t  Val A.: 0.658  CA.: 0.380  J.: 0.217\n",
      "[ 32] Loss: 0.951  \t  A.: 0.800  CA.: 0.582  J.: 0.303  \t  Val A.: 0.801  CA.: 0.588  J.: 0.360\n",
      "[ 33] Loss: 0.872  \t  A.: 0.813  CA.: 0.644  J.: 0.306  \t  Val A.: 0.752  CA.: 0.626  J.: 0.304\n",
      "[ 34] Loss: 0.846  \t  A.: 0.791  CA.: 0.597  J.: 0.304  \t  Val A.: 0.851  CA.: 0.653  J.: 0.409\n",
      "[ 35] Loss: 0.864  \t  A.: 0.811  CA.: 0.616  J.: 0.305  \t  Val A.: 0.843  CA.: 0.597  J.: 0.351\n",
      "[ 36] Loss: 0.785  \t  A.: 0.818  CA.: 0.627  J.: 0.325  \t  Val A.: 0.821  CA.: 0.565  J.: 0.306\n",
      "[ 37] Loss: 0.804  \t  A.: 0.824  CA.: 0.631  J.: 0.318  \t  Val A.: 0.855  CA.: 0.432  J.: 0.281\n",
      "[ 38] Loss: 0.772  \t  A.: 0.810  CA.: 0.629  J.: 0.312  \t  Val A.: 0.819  CA.: 0.577  J.: 0.310\n",
      "[ 39] Loss: 0.808  \t  A.: 0.837  CA.: 0.628  J.: 0.347  \t  Val A.: 0.812  CA.: 0.434  J.: 0.262\n",
      "[ 40] Loss: 0.873  \t  A.: 0.814  CA.: 0.646  J.: 0.329  \t  Val A.: 0.875  CA.: 0.681  J.: 0.394\n",
      "[ 41] Loss: 0.834  \t  A.: 0.825  CA.: 0.611  J.: 0.348  \t  Val A.: 0.861  CA.: 0.661  J.: 0.412\n",
      "[ 42] Loss: 0.702  \t  A.: 0.845  CA.: 0.642  J.: 0.325  \t  Val A.: 0.797  CA.: 0.471  J.: 0.248\n",
      "[ 43] Loss: 0.782  \t  A.: 0.815  CA.: 0.639  J.: 0.319  \t  Val A.: 0.799  CA.: 0.479  J.: 0.250\n",
      "[ 44] Loss: 0.865  \t  A.: 0.823  CA.: 0.609  J.: 0.323  \t  Val A.: 0.897  CA.: 0.603  J.: 0.409\n",
      "[ 45] Loss: 0.813  \t  A.: 0.827  CA.: 0.670  J.: 0.345  \t  Val A.: 0.887  CA.: 0.514  J.: 0.324\n",
      "[ 46] Loss: 0.627  \t  A.: 0.844  CA.: 0.661  J.: 0.330  \t  Val A.: 0.901  CA.: 0.666  J.: 0.393\n",
      "[ 47] Loss: 0.660  \t  A.: 0.851  CA.: 0.654  J.: 0.343  \t  Val A.: 0.866  CA.: 0.636  J.: 0.322\n",
      "[ 48] Loss: 0.916  \t  A.: 0.833  CA.: 0.621  J.: 0.296  \t  Val A.: 0.892  CA.: 0.640  J.: 0.355\n",
      "[ 49] Loss: 0.672  \t  A.: 0.856  CA.: 0.673  J.: 0.362  \t  Val A.: 0.816  CA.: 0.573  J.: 0.326\n",
      "[ 50] Loss: 0.764  \t  A.: 0.849  CA.: 0.680  J.: 0.352  \t  Val A.: 0.909  CA.: 0.651  J.: 0.407\n",
      "[ 51] Loss: 0.658  \t  A.: 0.846  CA.: 0.666  J.: 0.352  \t  Val A.: 0.880  CA.: 0.578  J.: 0.357\n",
      "[ 52] Loss: 0.649  \t  A.: 0.836  CA.: 0.703  J.: 0.320  \t  Val A.: 0.869  CA.: 0.581  J.: 0.356\n",
      "[ 53] Loss: 0.633  \t  A.: 0.852  CA.: 0.699  J.: 0.374  \t  Val A.: 0.824  CA.: 0.686  J.: 0.328\n",
      "[ 54] Loss: 0.656  \t  A.: 0.855  CA.: 0.686  J.: 0.333  \t  Val A.: 0.893  CA.: 0.665  J.: 0.406\n",
      "[ 55] Loss: 0.584  \t  A.: 0.850  CA.: 0.689  J.: 0.377  \t  Val A.: 0.901  CA.: 0.475  J.: 0.337\n",
      "[ 56] Loss: 0.646  \t  A.: 0.860  CA.: 0.689  J.: 0.362  \t  Val A.: 0.895  CA.: 0.512  J.: 0.315\n",
      "[ 57] Loss: 0.899  \t  A.: 0.821  CA.: 0.670  J.: 0.339  \t  Val A.: 0.859  CA.: 0.491  J.: 0.301\n",
      "[ 58] Loss: 0.669  \t  A.: 0.857  CA.: 0.668  J.: 0.346  \t  Val A.: 0.804  CA.: 0.543  J.: 0.294\n",
      "[ 59] Loss: 0.673  \t  A.: 0.857  CA.: 0.717  J.: 0.352  \t  Val A.: 0.843  CA.: 0.514  J.: 0.318\n",
      "[ 60] Loss: 0.658  \t  A.: 0.835  CA.: 0.707  J.: 0.366  \t  Val A.: 0.916  CA.: 0.648  J.: 0.434\n",
      "[ 61] Loss: 0.615  \t  A.: 0.846  CA.: 0.682  J.: 0.339  \t  Val A.: 0.881  CA.: 0.625  J.: 0.472\n",
      "[ 62] Loss: 0.509  \t  A.: 0.869  CA.: 0.720  J.: 0.391  \t  Val A.: 0.851  CA.: 0.457  J.: 0.336\n",
      "[ 63] Loss: 0.738  \t  A.: 0.862  CA.: 0.661  J.: 0.347  \t  Val A.: 0.884  CA.: 0.630  J.: 0.395\n",
      "[ 64] Loss: 0.673  \t  A.: 0.847  CA.: 0.688  J.: 0.355  \t  Val A.: 0.876  CA.: 0.539  J.: 0.331\n",
      "[ 65] Loss: 0.727  \t  A.: 0.812  CA.: 0.735  J.: 0.345  \t  Val A.: 0.920  CA.: 0.629  J.: 0.492\n",
      "[ 66] Loss: 0.562  \t  A.: 0.889  CA.: 0.718  J.: 0.393  \t  Val A.: 0.903  CA.: 0.698  J.: 0.341\n",
      "[ 67] Loss: 0.588  \t  A.: 0.858  CA.: 0.722  J.: 0.363  \t  Val A.: 0.842  CA.: 0.517  J.: 0.310\n",
      "[ 68] Loss: 0.559  \t  A.: 0.847  CA.: 0.687  J.: 0.363  \t  Val A.: 0.924  CA.: 0.563  J.: 0.399\n",
      "[ 69] Loss: 0.549  \t  A.: 0.844  CA.: 0.688  J.: 0.326  \t  Val A.: 0.913  CA.: 0.672  J.: 0.459\n",
      "[ 70] Loss: 0.585  \t  A.: 0.862  CA.: 0.680  J.: 0.385  \t  Val A.: 0.830  CA.: 0.630  J.: 0.339\n",
      "[ 71] Loss: 0.674  \t  A.: 0.852  CA.: 0.654  J.: 0.349  \t  Val A.: 0.893  CA.: 0.659  J.: 0.410\n",
      "[ 72] Loss: 0.746  \t  A.: 0.840  CA.: 0.720  J.: 0.380  \t  Val A.: 0.845  CA.: 0.673  J.: 0.379\n",
      "[ 73] Loss: 0.522  \t  A.: 0.874  CA.: 0.766  J.: 0.422  \t  Val A.: 0.858  CA.: 0.594  J.: 0.410\n",
      "[ 74] Loss: 0.748  \t  A.: 0.869  CA.: 0.675  J.: 0.403  \t  Val A.: 0.888  CA.: 0.479  J.: 0.355\n",
      "[ 75] Loss: 0.589  \t  A.: 0.844  CA.: 0.780  J.: 0.403  \t  Val A.: 0.901  CA.: 0.677  J.: 0.369\n",
      "[ 76] Loss: 0.632  \t  A.: 0.871  CA.: 0.719  J.: 0.376  \t  Val A.: 0.896  CA.: 0.507  J.: 0.362\n",
      "[ 77] Loss: 0.511  \t  A.: 0.857  CA.: 0.721  J.: 0.388  \t  Val A.: 0.859  CA.: 0.565  J.: 0.326\n",
      "[ 78] Loss: 0.450  \t  A.: 0.884  CA.: 0.710  J.: 0.408  \t  Val A.: 0.842  CA.: 0.605  J.: 0.348\n",
      "[ 79] Loss: 0.671  \t  A.: 0.856  CA.: 0.697  J.: 0.357  \t  Val A.: 0.912  CA.: 0.545  J.: 0.350\n",
      "[ 80] Loss: 0.575  \t  A.: 0.880  CA.: 0.723  J.: 0.404  \t  Val A.: 0.883  CA.: 0.681  J.: 0.394\n",
      "[ 81] Loss: 0.468  \t  A.: 0.877  CA.: 0.710  J.: 0.408  \t  Val A.: 0.872  CA.: 0.688  J.: 0.380\n",
      "[ 82] Loss: 0.611  \t  A.: 0.872  CA.: 0.717  J.: 0.409  \t  Val A.: 0.871  CA.: 0.683  J.: 0.389\n",
      "[ 83] Loss: 0.580  \t  A.: 0.878  CA.: 0.703  J.: 0.400  \t  Val A.: 0.878  CA.: 0.737  J.: 0.370\n",
      "[ 84] Loss: 0.636  \t  A.: 0.868  CA.: 0.733  J.: 0.387  \t  Val A.: 0.916  CA.: 0.728  J.: 0.433\n",
      "[ 85] Loss: 0.543  \t  A.: 0.879  CA.: 0.795  J.: 0.440  \t  Val A.: 0.928  CA.: 0.534  J.: 0.368\n",
      "[ 86] Loss: 0.522  \t  A.: 0.863  CA.: 0.765  J.: 0.409  \t  Val A.: 0.891  CA.: 0.648  J.: 0.429\n",
      "[ 87] Loss: 0.512  \t  A.: 0.869  CA.: 0.778  J.: 0.417  \t  Val A.: 0.865  CA.: 0.550  J.: 0.300\n",
      "[ 88] Loss: 0.682  \t  A.: 0.864  CA.: 0.754  J.: 0.374  \t  Val A.: 0.899  CA.: 0.538  J.: 0.344\n",
      "[ 89] Loss: 0.609  \t  A.: 0.860  CA.: 0.771  J.: 0.369  \t  Val A.: 0.851  CA.: 0.673  J.: 0.337\n",
      "[ 90] Loss: 0.560  \t  A.: 0.867  CA.: 0.769  J.: 0.382  \t  Val A.: 0.919  CA.: 0.449  J.: 0.372\n",
      "[ 91] Loss: 0.621  \t  A.: 0.865  CA.: 0.753  J.: 0.395  \t  Val A.: 0.840  CA.: 0.557  J.: 0.308\n",
      "[ 92] Loss: 0.592  \t  A.: 0.878  CA.: 0.748  J.: 0.399  \t  Val A.: 0.876  CA.: 0.557  J.: 0.332\n",
      "[ 93] Loss: 0.504  \t  A.: 0.849  CA.: 0.748  J.: 0.367  \t  Val A.: 0.905  CA.: 0.670  J.: 0.474\n",
      "[ 94] Loss: 0.442  \t  A.: 0.886  CA.: 0.777  J.: 0.438  \t  Val A.: 0.908  CA.: 0.601  J.: 0.337\n",
      "[ 95] Loss: 0.542  \t  A.: 0.880  CA.: 0.783  J.: 0.416  \t  Val A.: 0.907  CA.: 0.692  J.: 0.457\n",
      "[ 96] Loss: 0.511  \t  A.: 0.858  CA.: 0.809  J.: 0.404  \t  Val A.: 0.914  CA.: 0.701  J.: 0.455\n",
      "[ 97] Loss: 0.434  \t  A.: 0.876  CA.: 0.830  J.: 0.440  \t  Val A.: 0.881  CA.: 0.518  J.: 0.364\n",
      "[ 98] Loss: 0.647  \t  A.: 0.861  CA.: 0.780  J.: 0.410  \t  Val A.: 0.904  CA.: 0.737  J.: 0.412\n",
      "[ 99] Loss: 0.493  \t  A.: 0.856  CA.: 0.837  J.: 0.386  \t  Val A.: 0.889  CA.: 0.579  J.: 0.364\n"
     ]
    }
   ],
   "source": [
    "def accuracy(confusion):\n",
    "    # Overall pixelwise accuracy\n",
    "    # This metric heavily favors tiles and background (as they are most frequent)\n",
    "    return np.sum(np.diag(confusion)) / np.sum(confusion)\n",
    "\n",
    "def class_accuracy(confusion):\n",
    "    # Class wise accuracy\n",
    "    # This metric normalizes for class frequencies and favors small classes\n",
    "    return np.mean(np.diag(confusion) / (np.sum(confusion, axis=1) + 1e-10))\n",
    "\n",
    "def jaccard(confusion):\n",
    "    # Jaccard index\n",
    "    # A mix of the above, neither favors small or large classes much\n",
    "    D = np.diag(confusion)\n",
    "    return np.mean( D / (np.sum(confusion, axis=1) + np.sum(confusion, axis=0) - D + 1e-10))\n",
    "\n",
    "# Start a session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Set up training\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Run the training for some iterations\n",
    "for it in range(100):\n",
    "    sess.run(switch_train_op)\n",
    "\n",
    "    total_confusion = np.zeros((num_classes, num_classes))\n",
    "    loss_vals = []\n",
    "    # Run 10 training iterations and 1 validation iteration\n",
    "    for i in range(10):\n",
    "        confusion_val, loss_val, _ = sess.run([confusion, loss, opt])\n",
    "        total_confusion += confusion_val\n",
    "        loss_vals.append(loss_val)\n",
    "    \n",
    "    sess.run(switch_valid_op)\n",
    "    confusion_val = sess.run(confusion)\n",
    "\n",
    "    # Let's update tensorboard\n",
    "    summary_writer.add_summary( sess.run(merged_summary, {'loss:0': np.mean(loss_vals), 'accuracy:0': accuracy(total_confusion), 'class_accuracy:0': class_accuracy(total_confusion), 'jaccard:0': jaccard(total_confusion), 'val_accuracy:0': accuracy(confusion_val), 'val_class_accuracy:0': class_accuracy(confusion_val), 'val_jaccard:0': jaccard(confusion_val)}), it )\n",
    "    print('[%3d] Loss: %0.3f  \\t  A.: %0.3f  CA.: %0.3f  J.: %0.3f  \\t  Val A.: %0.3f  CA.: %0.3f  J.: %0.3f'%(it, np.mean(loss_vals), accuracy(total_confusion), class_accuracy(total_confusion), jaccard(total_confusion), accuracy(confusion_val), class_accuracy(confusion_val), jaccard(confusion_val)))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluation\n",
    "### Compute the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean class accuracy 0.71041832503\n"
     ]
    }
   ],
   "source": [
    "total_lbl, total_cor = np.zeros(6)+1e-10, np.zeros(6)\n",
    "for it in tf.python_io.tf_record_iterator('valid.tfrecord'):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(it)\n",
    "    I = np.frombuffer(example.features.feature['image_raw'].bytes_list.value[0], dtype=np.uint8).reshape(256, 256, 3)\n",
    "    L = np.frombuffer(example.features.feature['label_raw'].bytes_list.value[0], dtype=np.uint8).reshape(256, 256)\n",
    "    \n",
    "    P = sess.run('output:0', {'inputs:0':I[None]})\n",
    "    total_lbl += np.bincount(L.flat, minlength=6)\n",
    "    total_cor += np.bincount(L.flat, (P==L).flat, minlength=6)\n",
    "print( 'Mean class accuracy', np.mean(total_cor / total_lbl) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Save Model\n",
    "Please note that we also want you to turn in your ipynb for this assignment.  Zip up the ipynb along with the tfg for your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "util.save('assignment7.tfg', session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
