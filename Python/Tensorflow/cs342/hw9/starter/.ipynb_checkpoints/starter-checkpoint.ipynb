{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 9\n",
    "Let's predict the future.\n",
    "\n",
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import util\n",
    "\n",
    "# Colors to visualize the labeling\n",
    "COLORS = np.array([(0,0,0), (255,0,0), (0,255,0), (255,255,0), (0,0,255), (255,255,255)], dtype=np.uint8)\n",
    "CROP_SIZE = 64\n",
    "N_ACTION = 5\n",
    "\n",
    "offsets = [0, 6, 15, 30, 60, 120]\n",
    "\n",
    "def parser(record):\n",
    "    # Parse the TF record\n",
    "    \n",
    "    feature ={\n",
    "        'height': tf.FixedLenFeature([], tf.int64),\n",
    "        'width': tf.FixedLenFeature([], tf.int64),\n",
    "        'channels': tf.FixedLenFeature([], tf.int64),\n",
    "        'n_future': tf.FixedLenFeature([], tf.int64),\n",
    "        'action': tf.FixedLenFeature([], tf.int64),\n",
    "        'image_raw': tf.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    values = {'position' : np.float32, 'is_dying': np.int64, 'on_ground': np.int64, 'coins': np.int64}\n",
    "    for k,v in enumerate(values):\n",
    "        t = values[v]\n",
    "        for j, o in enumerate(offsets):\n",
    "            feature[v+'_%d'%j] = tf.FixedLenFeature([], t)\n",
    "\n",
    "    parsed = tf.parse_single_example(record, features=feature)\n",
    "     \n",
    "    # Load the data and format it\n",
    "    W = tf.cast(parsed['width'], tf.int32)\n",
    "    H = tf.cast(parsed['height'], tf.int32)\n",
    "    C = tf.cast(parsed['channels'], tf.int32)\n",
    "    A = tf.cast(parsed['action'], tf.int32)\n",
    "    actions = tf.stack([tf.bitwise.bitwise_and(A, (1<<i)) > 0 for i in range(N_ACTION)])\n",
    "    image = tf.reshape(tf.decode_raw(parsed[\"image_raw\"], tf.uint8), [H,W,C])\n",
    "    \n",
    "    current_position = tf.cast(parsed['position_0'], tf.float32)\n",
    "    current_is_dying = tf.cast(parsed['is_dying_0'], tf.bool)\n",
    "    current_coins = tf.cast(parsed['coins_0'], tf.float32)\n",
    "    \n",
    "    future_position = tf.stack([tf.cast(parsed['position_%d'%o], tf.float32) for o in range(1,len(offsets))])\n",
    "    future_is_dying = tf.stack([tf.cast(parsed['is_dying_%d'%o], tf.bool) for o in range(1,len(offsets))])\n",
    "    future_coins = tf.stack([tf.cast(parsed['coins_%d'%o], tf.float32) for o in range(1,len(offsets))])\n",
    "    \n",
    "    ## No data augmentation this time, as it might affect the future\n",
    "    return image, actions, current_position, current_is_dying, current_coins, future_position, future_is_dying, future_coins\n",
    "\n",
    "def load_dataset(tfrecord):\n",
    "    # Load the dataset\n",
    "    dataset = tf.contrib.data.TFRecordDataset(tfrecord)\n",
    "\n",
    "    # Parse the tf record entries\n",
    "    dataset = dataset.map(parser, num_threads=8, output_buffer_size=1024)\n",
    "\n",
    "    # Shuffle the data, batch it and run this for multiple epochs\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Define your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new log directory (if you run low on disk space you can either disable this or delete old logs)\n",
    "# run: `tensorboard --logdir log` to see all the nice summaries\n",
    "for n_model in range(1000):\n",
    "    LOG_DIR = 'log/model_%d'%n_model\n",
    "    from os import path\n",
    "    if not path.exists(LOG_DIR):\n",
    "        break\n",
    "\n",
    "# Lets clear the tensorflow graph, so that you don't have to restart the notebook every time you change the network\n",
    "tf.reset_default_graph()\n",
    "\n",
    "TF_COLORS = tf.constant(COLORS)\n",
    "\n",
    "train_data = load_dataset('future_train.tfrecord')\n",
    "valid_data = load_dataset('future_val.tfrecord')\n",
    "\n",
    "# Create an iterator for the datasets\n",
    "# The iterator allows us to quickly switch between training and validataion\n",
    "iterator = tf.contrib.data.Iterator.from_structure(train_data.output_types, ((None,64,64,9), (None,N_ACTION), (None,), (None,), (None,), (None,len(offsets)-1), (None,len(offsets)-1), (None,len(offsets)-1)))\n",
    "\n",
    "# and fetch the next images from the dataset (every time next_image is evaluated a new image set of 32 images is returned)\n",
    "image, action, current_position, current_is_dying, current_coins, future_position, future_is_dying, future_coins = iterator.get_next()\n",
    "\n",
    "# Define operations that switch between train and valid\n",
    "switch_train_op = iterator.make_initializer(train_data)\n",
    "switch_valid_op = iterator.make_initializer(valid_data)\n",
    "\n",
    "# Convert the input and label\n",
    "image = tf.identity(image, name='images')\n",
    "image = tf.cast(image, tf.float32)\n",
    "action = tf.identity(tf.cast(action, tf.int32), name='action')\n",
    "\n",
    "current_position = tf.identity(current_position, name='current_position')\n",
    "current_is_dying = tf.identity(current_is_dying, name='current_is_dying')\n",
    "current_coins = tf.identity(current_coins, name='current_coins')\n",
    "\n",
    "future_position = tf.identity(future_position, name='future_position')\n",
    "future_is_dying = tf.identity(future_is_dying, name='future_is_dying')\n",
    "future_coins = tf.identity(future_coins, name='future_coins')\n",
    "\n",
    "# Whiten the image\n",
    "white_image = (image - 100.) / 72.\n",
    "\n",
    "# TODO: Define your convnet and loss here\n",
    "# In preparation for the next homework you might want to make this network small and efficient.\n",
    "\n",
    "# Build the network out of a few convolutional layers\n",
    "\n",
    "# Hook up a fully connected layer to predict the action\n",
    "\n",
    "# Combine the action and the output of the conv layer\n",
    "\n",
    "# Use a cross entropy for the action and is_dying, L2 for position and coin.\n",
    "# TODO: define losses here\n",
    "\n",
    "loss = 0.05*coins_loss + position_loss + is_dying_loss + action_loss\n",
    "\n",
    "# a binary vector of size 5 for each image in the batch (DO NOT USE action TO PREDICT THIS!)\n",
    "pred_action = tf.identity(action_logit > 0.5, name='predicted_action')\n",
    "\n",
    "# vectors of size (5) for each image in the batch (use action AND current_position, current_is_dying, current_coins TO PREDICT THIS)\n",
    "# Hint: for some variables you might want to make them relative to current_..., for others not\n",
    "pred_is_dying = tf.identity(future_is_dying_logit > 0.5, name='predicted_is_dying')\n",
    "pred_position = tf.identity(future_position_logit, name='predicted_position')\n",
    "pred_coins = tf.identity(future_coins_logit, name='predicted_coins')\n",
    "\n",
    "action_acc = tf.reduce_mean(tf.cast(tf.equal(tf.cast(pred_action, tf.float32), tf.cast(action, tf.float32)), tf.float32))\n",
    "dying_acc = tf.reduce_mean(tf.cast(tf.equal(tf.cast(pred_is_dying, tf.float32), tf.cast(future_is_dying, tf.float32)), tf.float32))\n",
    "\n",
    "# Let's weight the regularization loss down, otherwise it will hurt the model performance\n",
    "# You can tune this weight if you wish\n",
    "regularization_loss = tf.losses.get_regularization_loss()\n",
    "total_loss = loss + 1e-6 * regularization_loss\n",
    "\n",
    "# Adam will likely converge much faster than SGD for this assignment.\n",
    "optimizer = tf.train.AdamOptimizer(0.001, 0.9, 0.999)\n",
    "\n",
    "# use that optimizer on your loss function (control_dependencies makes sure any \n",
    "# batch_norm parameters are properly updated)\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    opt = optimizer.minimize(total_loss)\n",
    "\n",
    "# Let's define some summaries for tensorboard\n",
    "tf.summary.image('image1', image[:,:,:,:3], max_outputs=3)\n",
    "tf.summary.image('image2', image[:,:,:,3:6], max_outputs=3)\n",
    "tf.summary.image('image3', image[:,:,:,6:9], max_outputs=3)\n",
    "tf.summary.scalar('action_loss', tf.placeholder(tf.float32, name='action_loss'))\n",
    "tf.summary.scalar('is_dying_loss', tf.placeholder(tf.float32, name='is_dying_loss'))\n",
    "tf.summary.scalar('position_loss', tf.placeholder(tf.float32, name='position_loss'))\n",
    "tf.summary.scalar('coins_loss', tf.placeholder(tf.float32, name='coins_loss'))\n",
    "tf.summary.scalar('loss', tf.placeholder(tf.float32, name='loss'))\n",
    "tf.summary.scalar('val_loss', tf.placeholder(tf.float32, name='val_loss'))\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR, tf.get_default_graph())\n",
    "\n",
    "# Let's compute the model size\n",
    "print( \"Total number of variables used \", np.sum([v.get_shape().num_elements() for v in tf.trainable_variables()]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training\n",
    "\n",
    "Training might take up to 20 min depending on your architecture (and if you have a GPU or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start a session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Set up training\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Run the training for some iterations\n",
    "for it in range(500):\n",
    "    sess.run(switch_train_op)\n",
    "\n",
    "    loss_vals = []\n",
    "    # Run 10 training iterations and 1 validation iteration\n",
    "    for i in range(10):\n",
    "        loss_val = sess.run([loss,action_loss,is_dying_loss,position_loss,coins_loss,opt])[:-1]\n",
    "        loss_vals.append(loss_val)\n",
    "    # Compute the summary\n",
    "    mean_loss = np.mean(np.array(loss_vals), axis=0)\n",
    "    summary = {n+':0':mean_loss[i] for i, n in enumerate(['loss','action_loss','is_dying_loss','position_loss','coins_loss'])}\n",
    "\n",
    "    # Compute the validation loss\n",
    "    sess.run(switch_valid_op)\n",
    "    loss_val = sess.run(loss)\n",
    "    summary['val_loss:0'] = loss_val\n",
    "    \n",
    "    summary_writer.add_summary( sess.run(merged_summary, summary), it )\n",
    "\n",
    "    # Let's update tensorboard\n",
    "    print('[%3d] Loss: %0.3f  \\t  val loss A.: %0.3f'%(it, mean_loss[0], loss_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluation\n",
    "### Compute the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(switch_valid_op)\n",
    "action_ac, dying_ac, pos_loss, coin_loss = sess.run([action_acc, dying_acc, position_loss, coins_loss])\n",
    "\n",
    "print('Action prediction accuracy: ' + str(action_ac))\n",
    "print('Dying prediction accuracy: ' + str(dying_ac))\n",
    "print('Position prediction L2: ' + str(pos_loss))\n",
    "print('Coin prediction L2: ' + str(coin_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Save Model\n",
    "Please note that we also want you to turn in your ipynb for this assignment.  Zip up the ipynb along with the tfg for your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "util.save('assignment9.tfg', session=sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
